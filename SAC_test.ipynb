{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SAC_test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNO7yzFA7fPi9NYYN59ZF/3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EconomistGrant/ReinforceTrade/blob/master/SAC_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rvbvz7WFHb_Y",
        "outputId": "c47d18d5-495e-4ef4-a1ba-7e26ec09d5f5"
      },
      "source": [
        "!git clone https://github.com/EconomistGrant/ReinforceTrade.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ReinforceTrade'...\n",
            "remote: Enumerating objects: 85, done.\u001b[K\n",
            "remote: Counting objects: 100% (85/85), done.\u001b[K\n",
            "remote: Compressing objects: 100% (58/58), done.\u001b[K\n",
            "remote: Total 85 (delta 27), reused 71 (delta 17), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (85/85), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sz-OeQhH47F"
      },
      "source": [
        "!sudo apt-get install -y xvfb ffmpeg\r\n",
        "!pip install 'imageio==2.4.0'\r\n",
        "!pip install matplotlib\r\n",
        "!pip install tf-agents[reverb]\r\n",
        "!pip install pybullet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0lpaOKsH_D9"
      },
      "source": [
        "import base64\r\n",
        "import imageio\r\n",
        "import IPython\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import os\r\n",
        "import reverb\r\n",
        "import tempfile\r\n",
        "import PIL.Image\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "from tf_agents.agents.ddpg import critic_network\r\n",
        "from tf_agents.agents.sac import sac_agent\r\n",
        "from tf_agents.agents.sac import tanh_normal_projection_network\r\n",
        "from tf_agents.metrics import py_metrics\r\n",
        "from tf_agents.networks import actor_distribution_network\r\n",
        "from tf_agents.policies import greedy_policy\r\n",
        "from tf_agents.policies import py_tf_eager_policy\r\n",
        "from tf_agents.policies import random_py_policy\r\n",
        "from tf_agents.replay_buffers import reverb_replay_buffer\r\n",
        "from tf_agents.replay_buffers import reverb_utils\r\n",
        "from tf_agents.train import actor\r\n",
        "from tf_agents.train import learner\r\n",
        "from tf_agents.train import triggers\r\n",
        "from tf_agents.train.utils import spec_utils\r\n",
        "from tf_agents.train.utils import strategy_utils\r\n",
        "from tf_agents.train.utils import train_utils\r\n",
        "\r\n",
        "tempdir = tempfile.gettempdir()\r\n",
        "\r\n",
        "from tf_agents.environments import utils\r\n",
        "import pandas as pd"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2h4KciEQIH8S"
      },
      "source": [
        "num_iterations = 100 # @param {type:\"integer\"}\r\n",
        "\r\n",
        "initial_collect_steps = 50 # @param {type:\"integer\"}\r\n",
        "collect_steps_per_iteration = 1 # @param {type:\"integer\"}\r\n",
        "replay_buffer_capacity = 10 # @param {type:\"integer\"}\r\n",
        "\r\n",
        "batch_size = 8 # @param {type:\"integer\"}\r\n",
        "\r\n",
        "critic_learning_rate = 3e-4 # @param {type:\"number\"}\r\n",
        "actor_learning_rate = 3e-4 # @param {type:\"number\"}\r\n",
        "alpha_learning_rate = 3e-4 # @param {type:\"number\"}\r\n",
        "target_update_tau = 0.005 # @param {type:\"number\"}\r\n",
        "target_update_period = 1 # @param {type:\"number\"}\r\n",
        "gamma = 0.99 # @param {type:\"number\"}\r\n",
        "reward_scale_factor = 1.0 # @param {type:\"number\"}\r\n",
        "\r\n",
        "actor_fc_layer_params = (256, 256)\r\n",
        "critic_joint_fc_layer_params = (256, 256)\r\n",
        "\r\n",
        "log_interval = 5000 # @param {type:\"integer\"}\r\n",
        "\r\n",
        "num_eval_episodes = 20 # @param {type:\"integer\"}\r\n",
        "eval_interval = 5 # @param {type:\"integer\"}\r\n",
        "\r\n",
        "policy_save_interval = 50 # @param {type:\"integer\"}"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdx-XTXyIJ5o"
      },
      "source": [
        "\r\n",
        "from ReinforceTrade.pyenv_single_action import TradingEnv\r\n",
        "from tf_agents.environments import tf_py_environment\r\n",
        "\r\n",
        "data = pd.read_csv('ReinforceTrade/environment/GSPC.csv')[['Date','Open','Close','High','Low']]\r\n",
        "data['Date'] = pd.to_datetime(data['Date'])\r\n",
        "data_input = data.values[:,1:]\r\n",
        "#data_input = np.array([[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0]])\r\n",
        "\r\n",
        "env = TradingEnv(data = data_input, rf = 0, back_looking = 4)\r\n",
        "utils.validate_py_environment(env, episodes=5)\r\n",
        "\r\n",
        "train_env = tf_py_environment.TFPyEnvironment(env)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArJp4AULIjMP",
        "outputId": "9960cb0e-e7af-4b44-eafe-0f2e7aea287a"
      },
      "source": [
        "use_gpu = True #@param {type:\"boolean\"}\r\n",
        "\r\n",
        "strategy = strategy_utils.get_strategy(tpu=False, use_gpu=use_gpu)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E091X13kIrMC"
      },
      "source": [
        "observation_spec, action_spec, time_step_spec = (\r\n",
        "      spec_utils.get_tensor_specs(train_env))\r\n",
        "\r\n",
        "with strategy.scope():\r\n",
        "  critic_net = critic_network.CriticNetwork(\r\n",
        "        (observation_spec, action_spec),\r\n",
        "        observation_fc_layer_params=None,\r\n",
        "        action_fc_layer_params=None,\r\n",
        "        joint_fc_layer_params=critic_joint_fc_layer_params,\r\n",
        "        kernel_initializer='glorot_uniform',\r\n",
        "        last_kernel_initializer='glorot_uniform')\r\n",
        "\r\n",
        "with strategy.scope():\r\n",
        "  actor_net = actor_distribution_network.ActorDistributionNetwork(\r\n",
        "      observation_spec,\r\n",
        "      action_spec,\r\n",
        "      fc_layer_params=actor_fc_layer_params,\r\n",
        "      continuous_projection_net=(\r\n",
        "          tanh_normal_projection_network.TanhNormalProjectionNetwork))\r\n",
        "  \r\n",
        "with strategy.scope():\r\n",
        "  train_step = train_utils.create_train_step()\r\n",
        "\r\n",
        "  tf_agent = sac_agent.SacAgent(\r\n",
        "        time_step_spec,\r\n",
        "        action_spec,\r\n",
        "        actor_network=actor_net,\r\n",
        "        critic_network=critic_net,\r\n",
        "        actor_optimizer=tf.compat.v1.train.AdamOptimizer(\r\n",
        "            learning_rate=actor_learning_rate),\r\n",
        "        critic_optimizer=tf.compat.v1.train.AdamOptimizer(\r\n",
        "            learning_rate=critic_learning_rate),\r\n",
        "        alpha_optimizer=tf.compat.v1.train.AdamOptimizer(\r\n",
        "            learning_rate=alpha_learning_rate),\r\n",
        "        target_update_tau=target_update_tau,\r\n",
        "        target_update_period=target_update_period,\r\n",
        "        td_errors_loss_fn=tf.math.squared_difference,\r\n",
        "        gamma=gamma,\r\n",
        "        reward_scale_factor=reward_scale_factor,\r\n",
        "        train_step_counter=train_step)\r\n",
        "\r\n",
        "  tf_agent.initialize()\r\n",
        "\r\n",
        "  \"motherfucker\""
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVAWgJYKI5fZ"
      },
      "source": [
        "table_name = 'uniform_table'\r\n",
        "table = reverb.Table(\r\n",
        "    table_name,\r\n",
        "    max_size=replay_buffer_capacity,\r\n",
        "    sampler=reverb.selectors.Uniform(),\r\n",
        "    remover=reverb.selectors.Fifo(),\r\n",
        "    rate_limiter=reverb.rate_limiters.MinSize(1))\r\n",
        "\r\n",
        "reverb_server = reverb.Server([table])\r\n",
        "\r\n",
        "reverb_replay = reverb_replay_buffer.ReverbReplayBuffer(\r\n",
        "    tf_agent.collect_data_spec,\r\n",
        "    sequence_length=2,\r\n",
        "    table_name=table_name,\r\n",
        "    local_server=reverb_server)\r\n",
        "\r\n",
        "dataset = reverb_replay.as_dataset(\r\n",
        "      sample_batch_size=batch_size, num_steps=2).prefetch(50)\r\n",
        "experience_dataset_fn = lambda: dataset"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kr39vGVTI9NQ"
      },
      "source": [
        "tf_eval_policy = tf_agent.policy\r\n",
        "eval_policy = py_tf_eager_policy.PyTFEagerPolicy(\r\n",
        "  tf_eval_policy, use_tf_function=True)\r\n",
        "\r\n",
        "tf_collect_policy = tf_agent.collect_policy\r\n",
        "collect_policy = py_tf_eager_policy.PyTFEagerPolicy(\r\n",
        "  tf_collect_policy, use_tf_function=True)\r\n",
        "\r\n",
        "random_policy = random_py_policy.RandomPyPolicy(\r\n",
        "  train_env.time_step_spec(), train_env.action_spec())\r\n"
      ],
      "execution_count": 19,
      "outputs": []
    }
  ]
}