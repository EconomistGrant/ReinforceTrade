写在前面：
当前版本有诸多不尽合理的地方，包括神经网络、超参数、选择action的写法以及底层的环境设置。但无论如何模型是搭起来了，周末计划多看几篇论文开始把不合理的地方修改起来。四月底出一份能正常学到东西的sample，五月把该模块化的东西模块化，就可以上线maintain了。

是的！拖了很久终于把强化学习网络安在量化投资环境里面了。拖了很久的原因主要是放弃了tensorflow来弄pytorch，而放弃tf的原因是本来想偷懒直接调包调参，实际上还是不如自己手动搭网络，而搭网络的api上torch确实比tf好多了。

总而言之，可以参考这篇pytorch官方的dqn教学：—————— 然后我来复盘一遍自己的dqn代码

# 回顾环境
原环境额可以参考我之前的推送。这个版本中我修改了一下环境，详细可见\environment\simple_trade.py。修改的主要原因是DQN算法只支持离散的动作，所以简单实现了九种action：卖100%，卖75%，卖50%，。。。一直到买100%。Reward Function还是简单定义为上期收益。整个类是继承自gym.Env的，api可以参考gym官网

# Go Through Code
首先整个环境的写法是参考pytorch官方dqn教学：ReplayBuffer基本上和原版没什么区别。

1. DQN网络
然后定义DQN网络，DQN的意义在于对输入的观测值（state），给各个动作（action）打分。如果state是离散的话，这个就是一个state-action Q table。如果state是连续的，就是一个函数，输入观测值，输出各个action的分数，而这个函数用神经网络搭起来。一开始想写CNN，后来感觉不是很有意义，因为CNN适合特征是同质的环境，比如NLP中的文字和图像中的像素点，而金融数据feed进去open high low closed的时间序列不同变量本质上是不一样的。不同变量本质上不一样就只能用一维的卷积神经网络，在每个变量的时间序列上滚动，我感觉怪怪的，不如直接把数据压扁成一列然后fully connected呢。所以我写了一个simple_DQN,三层神经网络，激活函数随便选的。

2. DQNAgent
再然后就是Agent Class。这个写法我也是到处参考，理论上其实github上随便搜几个项目大致的结构就懂了。act的话我用了两种写法，一种是epsilon-greedy，一种是用sm把最后输出的action values正则化然后采样，都偶尔能学点东西出来，但也都不稳定。其实核心的是optimize和train这两个函数，之后动刀估计也是从这两个函数动刀。

先讲train吧因为是api的接口。现在超参数也没有设计成熟，很多还没调到函数参数里面去。简单来说train函数的逻辑是外层episode循环，内层time step循环，同时统计total steps。
内层time step循环的话通过环境TradingEnv获取state（长度为观测窗口长度，宽度为观测变量数），转化为tensor，用policy_net获得action评分，然后选择action，再和TradingEnv交互获得reward。根据总步数和超参数更新policy_net和target_net，更新policy_net直接调用optimize，更新target_net就读取policy_net参数。

然后讲optimize函数。optimize从回忆缓存(近1000次的state,action,next_state,reward)中选取batch_size(32)的回忆,用policy_net输入state评估action，并选择值最大的action，即为V(s_t,a_t)。那么这个值如果学过一点马尔可夫过程MDP的朋友应该知道和r(s_t,a_t) + V(St+1)在理论上是相同的，所以我们用target_net去评估后者，并把两者在batch中的平方差和作为loss function，然后反向传播，云云。之所以要用两个神经网络去评估，是为了避免放大噪音，这个问题在噪音很大的金融数据里面是很严重的；理论上讲target_net是较早版本的policy_net,或者是学习速度较慢的policy_net。目前我感觉最大的问题就是在target_net和loss function上面，现在设定每2000步把target_net重置为policy_net,感觉有点问题；我每次evaluate的时候打印一次loss，发现会一直变小直到突然爆炸变大然后继续变小爆炸的循环，可能跟学习速度有一定关系。

不管怎么说，网络算是搭起来了，算是pytorch练手吧。下一步分两头走，其一要把代码整理一下，该抽象化的抽象化，该模块化的模块化，方便后期维护。其二要去回顾学过的课程、优质的论文和成熟的项目，看看别人神经网络怎么搭的，努力在月底前做出来一个能学到东西的算法吧，然后整一个jupyter notebook，就算项目成功上线了！

# 现在发现的问题/待理解和研究的情况
1. Policy Network Update
感觉不是很合理。应该有一些更好的做法，比如DDPG或者slow update这种。多学多看。
2. Loss function
这个loss的定义还是过于简单了
3. action
action的选择很复杂。现在的写法如上所述，但学不到东西可能跟这个有关。偶尔能学到，就是一直持仓100%到结束，感觉也不合理。之前我在华泰写的项目是只有三种动作，卖10%，买10%，不动。等模块化抽象化函数化之后可以不同的实现方式跑很多次记录比对一下数据，现在乱改参数就跟无头苍蝇一样


# 先把网络搞明白。
1. conv1d能保证变量之间足够的联动吗？
conv1d在向量上滑动，滑动之后对in_channels上的值求和

那么本质上选择什么作为channels，是想在时间序列上滑动然后在变量上求和，还是在变量上滑动在时间序列上求和

在图像处理上，channels是RGB，每张图像的RGB二维数据上滑动，然后对RGB三个in_channels求和

2. 是不是要把后两个变量顺序换一下？因为感觉好像in_channels比较适合不同的变量吗？因为这个在2d里面对应的是RGB

test_obs.shape = (1,4,6)
1 batch
4 in channels
6 length?
