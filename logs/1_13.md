# ReinforceTrade Step 1: Gym Environment
其实这段时间花在这个项目上的时间还是不太多，其一是因为前段时间签了合同又过圣诞元旦的懒了几周，其二又是因为最近IAP的项目开始做了，其三也是因为转战PUBG投入了不少时间（电竞人！）。至于能够还算按计划完成这个部分，完全是因为Gym这个环境太好写了，以及网上可供参考的自建Gym Environment Class太多了。我实在是只做了一点微小的贡献，算上注释不到200行就写成了一个最基础的强化学习环境。

虽然它简单，但还是解决了一些我在之前实习时棘手的一些痛点和上了6.036 Machine Learning之后思考出的一些东西。

1. 将当前持仓量和净值feed进强化学习Agent的观测值。这个问题当时在华泰的时候弄了很久也没搞定，跟老板请教了一下也不知道怎么动手，最后用的办法运行效率实在很低。究其根本是因为那个框架网上照搬的不是我写的，而且TensorFlow1.13的API实在是nmb太垃圾了（好吧是我太菜了）。至于为什么要实现这个，其一是做到更大程度的模拟，毕竟交易员在交易的时候肯定会考虑到当前策略运行的情况；其二是因为如果不观测持仓量，每个action要么是交易要么是下周期持仓，前者会让强化学习摸不清楚reward和action的关系（因为收益来源可能是很久之前的交易），后者会让强化学习不知道如何减少交易量以降低成本，结果是每个时间点都交易像疯子一样。强化学习的优势在于做决策，深度学习的优势在于感知，observation的定义和修改做的比较灵活是我重构这个项目的本质目的

2. 实现了back_looking/rolling_window的概念。之前所有的强化学习项目，无论是TensorTrade还是coursera网课里的项目，都只feed进上一个周期的观测值，我觉得挺不合理的，这样会让机器变的非常短视，抓不到变化趋势，尤其是学了CNN/RNN之后，引入一个缓冲器的概念我觉得还是非常有必要的。现在在环境定义的时候就可以定义让机器感知过去多少个周期的数据，之后也许会修改引入固定周期比如上周+上个月+去年的值，或者是再引入MA/EWMA这些概念。反正这个写起来还是很快的，参考我_observe()和past_nav,past_holdings这些buffer的写法。

3. pandas实在是太垃圾了。numpy不香吗？

整个环境的逻辑是一开始会调用reset,把用于缓冲和记录的容器全部初始化，定位到初始行，产生初始观测(_observe)。
while True:
    agent根据神经网络的算法(测试程序写的是随机取样)在action space中产生action；
    调用step函数，首先_execute_trade并_validate_trade，更新容器数据;
    产生reward;
    step++, 产生新观测值(_observe);
    判断是否结束episode(到了数据末尾/没钱了), break

之后要做的事情还是蛮多，争取月底搞一个tf-agent算法弄上刚写的这个环境，tf2.x对gym的调用整清楚，估计先从魔改一个官方tutorial开始吧。争取二月把TF学明白一点，这样开学之后有空就调调参，看看论文，搞搞不同算法/不同参数/不同环境定义（交易限制/风险控制）下的表现，写写记录，不定期更新一下xx新算法的表现，一想到就感觉很舒服



