# ReinforceTrade #2: PyEnvironment， Q-Network和Random Policy初运行
两个多礼拜没更新这个项目了，主要是Fin Lab的项目花了比想象中多的时间，以及遇到一些瓶颈。

首先就是TFAgent里面对自定义Gym环境的支持并不是特别理想，好像用Gym比较常用的是直接调用Gym已经设定好的几个环境，像著名的CartPole。而且本质上要用一个Wrapper把Gym环境转变为一个叫PyEnvironment的东西，所以我干脆直接去找了PyEnvironment的文档，发现也没有特别难写，于是花了一些时间把之前写好的Gym Environment改写成了PyEnvironment。
(官方文档：https://www.tensorflow.org/agents/api_docs/python/tf_agents/environments/py_environment/PyEnvironment)
(Example：https://github.com/tensorflow/agents/blob/master/tf_agents/environments/examples/tic_tac_toe_environment.py)
(我改写好的环境文档：https://github.com/EconomistGrant/ReinforceTrade/blob/master/pyenv.py)

有了环境文档之后就可以调用算法进行学习了！整体上看的话TFAgent如我预期一样调用算法是非常方便的，连我这种代码能力一般的朋友也能看明白。

# 初始化PyEnvioronment 环境实例，检验并转换成tensor图
首先是初始化环境，调用自己写好的PyEnvironment，数据是随便拉了一年日频的标普指数，back_looking是指每次能观测到多久之前的数据，每行的数据包括当天的open close high low和策略持仓、策略净值。
调用TF/utils检验环境能用，这里检验的主要是检查time_step数据格式和time_step_spec数据格式相同。这里补充一下，PyEnvironment里面最基础的“数据格式”是一个TimeStep实例，包括1)step_type(首次观测，正常，末次观测) 2)reward 3)discout 4) observation

然后再调用tf_py_environment把环境转换成tensor图。其实这一步是真正的精髓，因为我之前做的1.13的tensorflow完全要手写tensor图，真的是晕了，完全像是学一个全新的语言。。。
```python
env = TradingEnv(data = data_input, rf = 0, back_looking = 4)
utils.validate_py_environment(env, episodes=5)
train_env = tf_py_environment.TFPyEnvironment(env)
```
# 初始化Agent
然后就是设定超参数并初始化Agent。为了方便跑起来我就用了最基础的DQN，都是参考TF-Agent案例做的
(DQN tutorial: https://github.com/tensorflow/agents/blob/master/docs/tutorials/1_dqn_tutorial.ipynb)

这里遇到了一个小问题，DQN只支持一维的action，但是我之前的写法是二维的，一个维度确定交易类型一个维度确认交易大小。为了跑通简单改写了一个pyenv_single_action，0-1就是sell，1-2就是hold，2-3就是buy，交易大小就是距离左边界的大小。
```python
#%% Hyperparameters
num_iterations = 100 # @param {type:"integer"}

initial_collect_steps = 100  # @param {type:"integer"} 
collect_steps_per_iteration = 1  # @param {type:"integer"}
replay_buffer_max_length = 100000  # @param {type:"integer"}

batch_size = 4  # @param {type:"integer"}
learning_rate = 1e-3  # @param {type:"number"}
log_interval = 200  # @param {type:"integer"}

num_eval_episodes = 10  # @param {type:"integer"}
eval_interval = 10  # @param {type:"integer"}\
    
#%% Agent
q_net = q_network.QNetwork(
    train_env.observation_spec(),
    train_env.action_spec(),
    fc_layer_params=(100,))

agent = dqn_agent.DqnAgent(
    train_env.time_step_spec(),
    train_env.action_spec(),
    q_network=q_net,
    optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate),
    td_errors_loss_fn=common.element_wise_squared_loss,
    train_step_counter=tf.Variable(0))

agent.initialize()

#%% Policies
eval_policy = agent.policy
collect_policy = agent.collect_policy
random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),
                                                train_env.action_spec())
```

# RandomPolicy跑通
最后就是简单跑一个RandomPolicy随机策略，代码跑通了，虽然完全没开始“学习”，能跑通并算出average return。下一步的计划是研究一下算法和Policy，争取跑通一个算法(之前比较熟悉的是PPO和A2C?)并实现增长且收敛的学习曲线。如果有时间的话研究一下CPU/GPU，每次运行的时候终端给我返回一大堆东西好像是调用上GPU了但是也不能确定的样子。。。。。
```python
def compute_avg_return(environment, policy, num_episodes=10):

  total_return = 0.0
  for _ in range(num_episodes):

    time_step = environment.reset()
    episode_return = 0.0

    while not time_step.is_last():
      action_step = policy.action(time_step)
      time_step = environment.step(action_step.action)
      episode_return += time_step.reward
    total_return += episode_return

  avg_return = total_return / num_episodes
  return avg_return.numpy()[0]

for _ in range(5):
    print(compute_avg_return(train_env, random_policy, num_eval_episodes))
```


